{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from crawlers.tools import get_html_doc\n",
    "from dateutil.parser import parse\n",
    "import time\n",
    "import unidecode\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://ventureburn.com/2017/05/fincluders-startup-challenge-amman-winners-announced/', 'http://ventureburn.com/?post_type=post&p=118282', 'http://ventureburn.com/2017/05/seedstars-world-heads-south-africa/', 'http://ventureburn.com/2017/05/tackle-infrastructure-skills-challenges-boost-tech-sector-say-nigerian-experts/', 'http://ventureburn.com/2017/05/top-startup-events-south-africa-africa-know-week-08042017/', 'http://ventureburn.com/2017/05/zimbabwe-government-applications-25-million-fund/', 'http://ventureburn.com/2017/05/ccdi-back-seven-innovative-cape-projects-second-round-seed-fund/', 'http://ventureburn.com/2017/05/reserve-bank-looking-unblock-ecosystem-challenges-winde/', 'http://ventureburn.com/2017/05/gsma-announces-top-startups-ecosystem-accelerator-innovation-fund/', 'http://ventureburn.com/2017/05/world-economic-forum-announces-top-female-tech-entrepreneurs/', 'http://ventureburn.com/2017/05/chance-meeting-helped-sa-startup-getsmarter-secure-100m-sale/']\n"
     ]
    }
   ],
   "source": [
    "link = \"http://ventureburn.com/\"\n",
    "html_doc = get_html_doc(link)\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "blocks = soup.select(\"ul.archive-list li div.archive-text h2 a\")\n",
    "links = []\n",
    "for block in blocks:\n",
    "    link = block[\"href\"]\n",
    "    links.append(link)\n",
    "print(links)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historically, the UK has struggled to found cybersecurity companies but that has recently started to turn change. What defines a pure startup is contentious but here we’ve decided to limit our horizon to the last five years (roughly since 2011) in an effort to sift the fresh from the established and to capture startups that truly count as ‘cyber’ and not only security.  In he past, celever engineers went to work for GCHQ - will that still be true in the future? \n"
     ]
    }
   ],
   "source": [
    "link = \"http://www.techworld.com/picture-gallery/security/uks-10-most-promising-cybersecurity-startups-2016-3634620/\"\n",
    "html_doc = get_html_doc(link)\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "content =\"\"\n",
    "group = soup.select_one(\"div.titleGroup\")\n",
    "paragraphs = group.find_all(\"p\",recursive=False)\n",
    "\n",
    "for para in paragraphs:\n",
    "    text = para.getText()\n",
    "    if \"Read next:\" in text:\n",
    "        text=\"\"\n",
    "    content = content+text+' '\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from crawlers.tools import get_html_doc\n",
    "from dateutil.parser import parse\n",
    "import time\n",
    "import unidecode\n",
    "\n",
    "\n",
    "class CrawlerVentureBurn:\n",
    "    def __init__(self, number_of_pages_to_crawl):\n",
    "\n",
    "        self.articles = []\n",
    "        self.pages = []\n",
    "        self.name =\"VentureBurn\"\n",
    "        base_url = \"http://ventureburn.com/page/\"\n",
    "        for x in range(1, number_of_pages_to_crawl + 1):\n",
    "            self.pages.append(base_url + str(x))\n",
    "        \n",
    "        \n",
    "    def crawl(self):\n",
    "        pages_length = len(self.pages)\n",
    "        for idx, page in enumerate(self.pages):\n",
    "            links = []\n",
    "            html_doc = get_html_doc(page)\n",
    "            soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "            blocks = soup.select(\"ul.archive-list li div.archive-text h2 a\")\n",
    "            for block in blocks:\n",
    "                links.append(block[\"href\"])\n",
    "\n",
    "            for link in links:\n",
    "                html_doc = get_html_doc(link)\n",
    "                soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "                paragraphs = soup.select(\"div#home-main p\")\n",
    "                if len(paragraphs)==0:\n",
    "                    paragraphs = soup.select(\"p\")                    \n",
    "                content = \"\"\n",
    "                for para in paragraphs:\n",
    "                    text = para.getText()\n",
    "                    content = content+text+' '\n",
    "                    content = unidecode.unidecode(content)\n",
    "                            \n",
    "                    \n",
    "                title = soup.select_one(\"div.story-headline h1\").getText()\n",
    "                title = unidecode.unidecode(title)\n",
    "                date  = soup.find(\"meta\",  property=\"article:published_time\")[\"content\"]\n",
    "                date  = int(parse(date).timestamp()) \n",
    "\n",
    "                article = {\n",
    "                    \"title\":title,\n",
    "                    \"content\":content,\n",
    "                    \"date\":date,\n",
    "                    \"url\":link,\n",
    "                    \"origin\":\"VentureBurn\"\n",
    "                }\n",
    "                self.articles.append(article)\n",
    "            \n",
    "            print(\"VentureBurn crawling at \" + str(((idx + 1) / pages_length) * 100) + \"%\")\n",
    "            \n",
    "    def get_articles(self):\n",
    "        return self.articles\n",
    "            \n",
    "                    \n",
    "                \n",
    "             \n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from crawlers.tools import get_html_doc\n",
    "from dateutil.parser import parse\n",
    "import time\n",
    "import unidecode\n",
    "\n",
    "\n",
    "class CrawlerVentureBurn:\n",
    "    def __init__(self, number_of_pages_to_crawl):\n",
    "\n",
    "        self.articles = []\n",
    "        self.pages = []\n",
    "        self.name =\"VentureBurn\"\n",
    "        base_url = \"http://ventureburn.com/page/\"\n",
    "        for x in range(1, number_of_pages_to_crawl + 1):\n",
    "            self.pages.append(base_url + str(x))\n",
    "        \n",
    "        \n",
    "    def crawl(self):\n",
    "        pages_length = len(self.pages)\n",
    "        for idx, page in enumerate(self.pages):\n",
    "            links = []\n",
    "            html_doc = get_html_doc(page)\n",
    "            soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "            blocks = soup.select(\"ul.archive-list li div.archive-text h2 a\")\n",
    "            for block in blocks:\n",
    "                links.append(block[\"href\"])\n",
    "\n",
    "            for link in links:\n",
    "                html_doc = get_html_doc(link)\n",
    "                soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "                paragraphs = soup.select(\"div#home-main p\")\n",
    "                if len(paragraphs)==0:\n",
    "                    paragraphs = soup.select(\"p\")                    \n",
    "                content = \"\"\n",
    "                for para in paragraphs:\n",
    "                    text = para.getText()\n",
    "                    content = content+text+' '\n",
    "                    content = unidecode.unidecode(content)\n",
    "                            \n",
    "                    \n",
    "                title = soup.select_one(\"div.story-headline h1\").getText()\n",
    "                title = unidecode.unidecode(title)\n",
    "                date  = soup.find(\"meta\",  property=\"article:published_time\")[\"content\"]\n",
    "                date  = int(parse(date).timestamp()) \n",
    "\n",
    "                article = {\n",
    "                    \"title\":title,\n",
    "                    \"content\":content,\n",
    "                    \"date\":date,\n",
    "                    \"url\":link,\n",
    "                    \"origin\":\"VentureBurn\"\n",
    "                }\n",
    "                self.articles.append(article)\n",
    "            \n",
    "            print(\"VentureBurn crawling at \" + str(((idx + 1) / pages_length) * 100) + \"%\")\n",
    "            \n",
    "    def get_articles(self):\n",
    "        return self.articles\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VentureBurn crawling at 33.33333333333333%\n",
      "VentureBurn crawling at 66.66666666666666%\n",
      "VentureBurn crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "Crawler= CrawlerVentureBurn(3)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testVentureBeat.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'startup'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile('\\W')\n",
    "re.sub( pattern, '', 'start-up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
