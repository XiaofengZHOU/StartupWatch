{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data\n",
    "  \n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5242, 3083, 12, 6, 195, 2, 3136, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Company RefME/NNP)\n",
      "(Company RefMe/NNP)\n",
      "(Company PS3.75/NNP)\n",
      "(Company April/NNP)\n",
      "(Company GEMS/NNP Global/NNP)\n",
      "(Company Us/NNP)\n",
      "(Company PS15/NNP)\n",
      "(Company BBC/NNP)\n",
      "(Company Daniel/NNP Hirschmann/NNP)\n",
      "(Company Bethany/NNP Koby/NNP)\n",
      "(Company CEO/NNP Koby/NNP)\n",
      "(Company UK/NNP)\n",
      "(Company Us/NNP)\n",
      "(Company PS1.2/NNP)\n",
      "(Company Saatchinvest/NNP)\n",
      "(Company December/NNP)\n",
      "(Company George/NNP Burgess/NNP)\n",
      "(Company Stanford/NNP University/NNP)\n",
      "(Company Gojimo/NNP)\n",
      "(Company Android/NNP)\n",
      "(Company McGraw-Hill/NNP Education/NNP)\n",
      "(Company Oxford/NNP University/NNP Press/NNP)\n",
      "(Company Gojimo/NNP)\n",
      "(Company PS630,000/NNP)\n",
      "(Company Index/NNP Ventures/NNP)\n",
      "(Company JamJar/NNP Investments/NNP)\n",
      "(Company Drinks/NNP)\n",
      "(Company Krishan/NNP Meetoo/NNP)\n",
      "(Company Carl/NNP Dawson/NNP)\n",
      "(Company Proversity/NNP)\n",
      "(Company Meetoo/NNP)\n",
      "(Company Proversity/NNP)\n",
      "(Company PS1/NNP)\n",
      "(Company Czech/NNP Republic/NNP)\n",
      "(Company RSBC/NNP Venture/NNP Capital/NNP)\n",
      "(Company November/NNP)\n",
      "(Company Series/NNP A/NNP)\n",
      "(Company USA/NNP)\n",
      "(Company Middle/NNP East/NNP)\n",
      "(Company ASEAN/NNP)\n",
      "(Company Rosetta/NNP Stone/NNP)\n",
      "(Company UK/NNP)\n",
      "(Company Memrise/NNP)\n",
      "(Company Memrise/NNP)\n",
      "(Company PS4.5/NNP)\n",
      "(Company Series/NNP A/NNP)\n",
      "(Company London/NNP VC/NNP)\n",
      "(Company Balderton/NNP Capital/NNP)\n",
      "(Company Firefly/NNP)\n",
      "(Company Firefly/NNP)\n",
      "(Company UK/NNP)\n",
      "(Company PS3,000/NNP)\n",
      "(Company PS10,000/NNP)\n",
      "(Company Show/NNP My/NNP Homework/NNP)\n",
      "(Company Show/NNP My/NNP Homework/NNP)\n",
      "(Company PS2.4/NNP)\n",
      "(Company LocalGlobe/NNP)\n",
      "(Company January/NNP)\n",
      "(Company Wonde/NNP)\n",
      "(Company Wonde/NNP)\n",
      "(Company Wonde/NNP)\n",
      "(Company API/NNP)\n",
      "(Company Knowledgemotion/NNP)\n",
      "(Company Founder/NNP David/NNP Bainbridge/NNP)\n",
      "(Company Techworld/NNP)\n",
      "(Company Knowledgemotion/NNP)\n",
      "(Company Chromebooks/NNP)\n",
      "(Company Pearson/NNP)\n",
      "(Company Bainbridge/NNP)\n",
      "(Company API/NNP)\n",
      "(Company Knowledgemotion/NNP)\n",
      "(Company PS1.2/NNP)\n",
      "(Company ICG/NNP Ventures/NNP)\n",
      "(Company Ingram/NNP Content/NNP Group/NNP)\n",
      "(Company Series/NNP A/NNP)\n",
      "(Company Pobble/NNP)\n",
      "(Company Anthony/NNP Horowitz/NNP)\n",
      "(Company Michael/NNP Morpurgo/NNP)\n",
      "(Company PS170,000/NNP)\n",
      "(Company CrowdCube/NNP)\n",
      "(Company February/NNP)\n",
      "(Company Digital/NNP Assess/NNP)\n",
      "(Company Digital/NNP Assess/NNP)\n",
      "(Company Goldsmiths/NNP)\n",
      "(Company Eton/NNP)\n",
      "(Company Digital/NNP Assess/NNP)\n",
      "(Company PS2.25/NNP)\n",
      "(Company Nesta/NNP Impact/NNP Investments/NNP)\n",
      "(Company June/NNP)\n",
      "(Company CRM/NNP)\n",
      "(Company SEO/NNP)\n",
      "(Company Fluency/NNP)\n",
      "(Company Fluency/NNP)\n",
      "(Company PS82,000/NNP)\n",
      "(Company Bethnal/NNP Green/NNP Ventures/NNP)\n",
      "(Company Clearly/NNP Social/NNP Angels/NNP)\n",
      "(Company AVADO/NNP)\n",
      "(Company Lisa/NNP Barrett/NNP)\n",
      "(Company AVADO/NNP)\n",
      "(Company Techworld/NNP)\n",
      "(Company AVADO/NNP)\n",
      "(Company Dot/NNP Native/NNP)\n",
      "(Company AVADO/NNP)\n",
      "(Company Moodle/NNP)\n",
      "(Company VC/NNP)\n",
      "(Company Blenheim/NNP Chalcot/NNP)\n",
      "(Company Oxford/NNP University/NNP)\n",
      "(Company Shameer/NNP Thobhani/NNP)\n",
      "(Company Lectus/NNP)\n",
      "(Company PS0.40/NNP)\n",
      "(Company Imperial/NNP College/NNP London/NNP)\n",
      "(Company Oxford/NNP)\n",
      "(Company Filippo/NNP Yacob/NNP)\n",
      "(Company Cubetto/NNP)\n",
      "(Company Cubetto/NNP)\n",
      "(Company Colour/NNP)\n",
      "(Company Cubetto/NNP)\n",
      "(Company Primo/NNP Toys/NNP)\n",
      "(Company PS1.3/NNP)\n",
      "(Company Kickstarter/NNP)\n",
      "(Company Cubetto/NNP)\n",
      "(Company PS159/NNP)\n",
      "(Company Code/NNP Kingdoms/NNP)\n",
      "(Company JavaScript/NNP)\n",
      "(Company Ross/NNP Targett/NNP)\n",
      "(Company Hugh/NNP Collins/NNP)\n",
      "(Company Entrepreneur/NNP First/NNP)\n",
      "(Company PS280,000/NNP)\n",
      "(Company SparkLabs/NNP Global/NNP Ventures/NNP)\n",
      "(Company EF/NNP)\n",
      "(Company Entrepreneur/NNP First/NNP)\n",
      "(Company AngelLab/NNP)\n",
      "(Company Neon/NNP Adventures/NNP)\n",
      "(Company Code/NNP Club/NNP)\n",
      "(Company Code/NNP Kingdoms/NNP)\n",
      "(Company Code/NNP Club/NNP)\n",
      "(Company UK/NNP)\n",
      "(Company Raspberry/NNP Pi/NNP Foundation/NNP)\n",
      "(Company Code/NNP Club/NNP)\n",
      "(Company Scratch/NNP)\n",
      "(Company HTML/NNP)\n",
      "(Company CSS/NNP)\n",
      "(Company Python/NNP)\n",
      "(Company Tutora/NNP)\n",
      "(Company Scott/NNP Woodley/NNP)\n",
      "(Company Mark/NNP Hughes/NNP)\n",
      "(Company August/NNP)\n",
      "(Company Stripe/NNP)\n",
      "(Company UK/NNP)\n",
      "(Company PS15/NNP)\n",
      "(Company ID/NNP)\n",
      "(Company Tutora/NNP)\n",
      "(Company PS150,000/NNP)\n",
      "(Company April/NNP)\n",
      "(Company Series/NNP A/NNP)\n",
      "(Company Blackbullion/NNP)\n",
      "(Company Blackbullion/NNP)\n",
      "(Company >/NNP)\n",
      "(Company >/NNP)\n",
      "(Company TechHub/NNP)\n",
      "(Company Google/NNP Campus/NNP)\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "  Company:{<NNP>+}    # chunk determiner/possessive, adjectives and noun\n",
    "           \n",
    "                         # chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "\n",
    "str1 = \"\"\"\n",
    "RefME takes the tedium out of compiling citations - and is proving extremely popular with students. The free app works by scanning the book you want to reference to speed up the task of creating, formatting and managing bibliographies.  RefMe raised $5 million (PS3.75 million) in seed round funding in April 2015, led by GEMS Global. This Hackney-based edtech startup blends hardware and software skills for children by selling toys that kids can build and code themselves. Technology Will Save Us has kits for building your own synthesiser, speaker, games console and even a starter soldering kit, with prices starting at PS15 for the BBC micro:bit. The company was dreamed up by husband and wife Daniel Hirschmann and Bethany Koby around their kitchen table in 2012. CEO Koby said last year: \"I'm not sure there has ever been a more exciting time to be building a learning-focused organisation in the UK, and we are thrilled to be leading the charge and championing edtech in this field.\" Technology Will Save Us raised a PS1.2 million seed funding round led by Saatchinvest in December 2015. George Burgess is a 23-year-old British entrepreneur who dropped out from Stanford University to focus on growing edtech startup Gojimo. The exam revision app is available on iOS, Android and the web and offers over 50,000 free curriculum-specific quiz questions to over 300,000 monthly active users. Premium content from the likes of McGraw-Hill Education and Oxford University Press is also available as in-app purchases.  Gojimo previously raised over $1 million (PS630,000) in a seed round led by Index Ventures, which included participation by JamJar Investments (the innocent Drinks founders). Krishan Meetoo, along with his co-founder Carl Dawson, set up Proversity to provide a learning and development environment \"across the employee lifecycle, from attracting talent through to on boarding them by providing the professional and soft skills they need for the role. So try before you buy with candidates,\" Meetoo said. Proversity is working in the corporate learning space, providing an on-demand mobile platform that brings employee attraction, recruitment and retention into one place. Candidates are given a structured learning programme, with each course being designed on a bespoke basis depending on the employer needs. Content can range from text, photos, motion graphics, video walkthroughs and various forms of assessment, from essays to simple check boxes. Proversity raised $1.6 million (PS1 million) in investment from Czech Republic fund RSBC Venture Capital in November 2015 and is planning a PS5-8 million Series A round this year as it expands into the USA, Middle East and ASEAN markets. Learning a new language is really hard, and companies have been trying to make it easier as far back as those giant Rosetta Stone software boxes were being advertised on the TV. UK startup Memrise claims to have cracked the science behind learning and retaining a new language, called 'elaborate encoding.' The platform then reinforces learning by testing and scheduling reminders to keep participants engaged. Memrise has raised more than $6 million (PS4.5 million) in seed and Series A funding so far, led by London VC firm Balderton Capital. London-based Firefly aims to free up teachers' time by simplifying the processes of sifting e-mail for homework, working with a clunky virtual learning environment or copying data into multiple apps that don't communicate. The portal integrates with existing school systems to bring everything into one place. Teachers can issue homework, give feedback and track progress in one place. For students this means being able to submit work at any time, so not having to hand in work while on study leave, for example. It also gives parents an easy way to track their child's progress and timetables. Firefly is already used in 300 schools across the UK and typically charges between PS3,000 and PS10,000 depending on the number of pupils. The startup has so far avoided venture capital. A similar, well established startup is London-based Show My Homework. The clue is in the name, as the platform aims to give teachers their evenings back by bringing time saving reports, instant access to online resources and access to homework into one place. The app also gives parents total visibility of their child's homework and students a place to store and manage all of their homework in one place. Show My Homework is already in more than 1500 schools globally and it raised PS2.4 million in funding from venture capital firm LocalGlobe in January 2017. Wonde is a Cambridge-based startup working to help schools keep their data secure. The platform allows schools to view and manage their data as it is accessed by third party applications. Once in place Wonde enables administrators to allow or deny access to school data and manage existing applications. For developers Wonde supplies an API for access to school data without having to set up agreements with individual organisations. Knowledgemotion's primary product is the boclips platform, which allows education providers to find and embed over two million video clips into their teaching materials without having to deal with tricky licensing issues or multiple content providers. The service is available as a clips library or a white label portal, with usage charged either per stream, or at a flat rate per asset used. Founder David Bainbridge told Techworld that he created Knowledgemotion after asking himself: \"In an education world that is manifestly changing, where textbooks are becoming Chromebooks, why wasn't the content experience in classrooms - from schools to university to corporate training - catching up with the tech delivery opportunity that seemed to be unfolding?\" The company has already signed a supply deal with education content publishers Pearson. Bainbridge says this means \"clips are API integrated into their publishing tool. So when a textbook is being created the author can search and pull in the relevant clip to illustrate the paragraph.\" Knowledgemotion has raised close to PS1.2 million from angel investors and ICG Ventures - part of major textbook publisher Ingram Content Group - and is preparing for a Series A round at the time of writing. London-based startup Pobble is a platform for children's writing, with the aim of encouraging even the most reluctant young writers by opening up a global audience for their published work. There is a school version which can be embedded, with trackable metrics. Pobble claims to be used in 100 countries and has already seen 30,000 pieces published to the platform. Authors Anthony Horowitz and Michael Morpurgo are fans too. Pobble raised PS170,000 in crowdfunding with CrowdCube in 2015, and an additional PS900,000 funding round in February 2016. Digital Assess is trying to change the way students and assessors work together on feedback relating to pieces of work. Assessors can leave contextual feedback, so comments on a single line of music, a specific paragraph or a slide for a project, regardless of file format. The mobile platform lets work be presented as a \"storyboard-style digital portfolio.\" Digital Assess works predominantly with higher education bodies, including Goldsmiths university and Eton college, but also vocational learning and accreditation bodies. Digital Assess raised $3 million (PS2.25 million) from investors including Nesta Impact Investments in June 2015. Fluency focuses on online learning and development courses in digital employability skills, such as analytics, coding, email, CRM, SEO and social media. There are just 25 courses available on the free portal at the time of writing, with each one clearly marked for how long it will take to complete. The aim is that as candidates take on skills and learnings they become more visible to employers. The platform has started pulling through digital jobs so once candidates successfully pass a course they become eligible for jobs through Fluency. The platform also aims to help employers find skilled candidates by designing courses tailored to their needs. Fluency raised $110,000 (PS82,000) in angel investment from Bethnal Green Ventures and Clearly Social Angels back in 2014. London-based startup AVADO focuses on corporate e-learning, teaching employees digital skills through bespoke courses. Lisa Barrett, managing director at AVADO told Techworld that each customer starts out with a face to face meeting with AVADO, where they will \"do a capability audit or assessment around digital skills at the organisation to understand their vision and strategy and where the gaps are for digital ways of working,\" she said. One of these solutions is Dot Native, which allows employees to do short, video-driven learning tasks. \"A key challenge around e-learning is that it doesn't work, it's repetitive and people don't finish. So we have designed bite-sized pieces of learning,\" she said. AVADO builds its learning tools on a highly customised version of the open source Moodle learning technology. The startup has been backed by VC firm Blenheim Chalcot an undisclosed amount. Oxford University graduate Shameer Thobhani has launched Lectus, an iOS-only app which allows students to connect with expert tutors around the world via video calls. The app is still a little buggy and at the time of writing there are only a handful of tutors, charging PS0.40 a minute, but the premise of opening up elite tutors to a broader audience is extremely compelling. Current tutors include a second year biomedical science student at Imperial College London and a 22 year old history and english graduate from Oxford. Subjects include university admissions, languages, history, mathematics, science, economics and geography. Filippo Yacob came up with the idea of Cubetto - a coding toy designed to help children aged three years and up to write their first computer programs - after the birth of his son. Cubetto is a wooden robot which teaches kids to code using blocks instead of screens. Colour coded directional arrows can be placed on the interface board which will direct the Cubetto on a pre-defined path around maps that can be laid on the floor. This apparently teaches children the basics of algorithms, debugging, and recursions. It is a product of London-based startup Primo Toys which raised $1.6 million (PS1.3 million) for the project on Kickstarter last year. Cubetto isn't cheap, retailing at PS159. Teaching the next generation to code is the aim of many a startup, and Code Kingdoms takes a gamification approach, turning glitches into enemies that need to be destroyed. The game uses JavaScript to allow the target audience of 6-13 year olds to create fantasy worlds.  Founders Ross Targett and Hugh Collins came out of the Entrepreneur First graduate accelerator programme and the startup has raised a total of PS280,000 in seed funding from SparkLabs Global Ventures, EF (Entrepreneur First), AngelLab and Neon Adventures. Code Club is a partner of Code Kingdoms and is responsible for events and workshops for 9-11 year olds to learn about coding. Code Club is hosted by the UK charity Raspberry Pi Foundation to encourage more young people to learn coding skills and close the growing technology skills gap.  Kids that attend Code Club will be taught how to programme through fun applications like computer games, animations and websites. Courses start out using Scratch to learn the basics of programming, before graduating to basic web development using HTML and CSS and then advanced skills using established programming languages like Python. Sheffield-based startup Tutora was founded by ex-teacher Scott Woodley and technology analyst Mark Hughes in August 2015. It is a platform for helping parents and students find local, in-person tutors and pay for it without having to handle cash (using Stripe). There are currently around 5,000 tutors across 11 UK cities to choose from, starting at PS15 per hour. Tutors are vetted using their government ID and can be searched by subject, price, customer feedback (a star rating) and proximity. Tutora crowdfunded PS150,000 from 75 investors in April for a 10 percent share in the business. It is currently preparing for a Series A round. Blackbullion seeks to help students and young people take better care of their finances, which in turn helps improve employability. The Blackbullion school edition takes the form of an eight part story and the university version comes in short learning modules based around making informed financial decisions in a non-patronising way. now>press>play is an \"immersive educational resource\" that looks to help primary school children engage with the curriculum through immersive storytelling. In a now>press>play experience each child is given a pair of headphones which will tell an audio story that incorporates learnings, discovery and problem solving. Pricing is bespoke according to the school. Edalytics is based out of the TechHub at Google Campus and is currently in private beta mode with schools. The mysterious startup says that it is: \"Testing technologies such as gamification, cognitive studies, artificial intelligence and data mining to create a unique learning experience for the students.\" \n",
    "\"\"\"\n",
    "sentences =  nltk.sent_tokenize(str1)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    parser = cp.parse(pos)\n",
    "    for par in parser:\n",
    "        if type(par) == nltk.tree.Tree:\n",
    "            print(par)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']\n",
      "    labels: ['anarchism', 'as', 'a', 'originated', 'as', 'term', 'a', 'of']\n",
      "\n",
      "with num_skips = 4 and skip_window = 2:\n",
      "    batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']\n",
      "    labels: ['originated', 'a', 'anarchism', 'term', 'of', 'term', 'as', 'originated']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import json\n",
    "\n",
    "words_list = []\n",
    "f = open(\"data/data.json\",\"r\")\n",
    "sites = json.load(f)\n",
    "f.close()\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  Company:{<NNP>+}    # chunk determiner/possessive, adjectives and noun\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "pattern = re.compile(r'\\W')\n",
    "\n",
    "for key in sites.keys():\n",
    "    articles = sites[key]\n",
    "    for article in articles:\n",
    "        content = article[\"content\"]\n",
    "        \n",
    "        sentences = nltk.sent_tokenize(content)\n",
    "        for sentence in sentences:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            pos = nltk.pos_tag(tokens)\n",
    "            parsers = cp.parse(pos)\n",
    "            \n",
    "            for par in parsers:\n",
    "                if type(par) == nltk.tree.Tree:\n",
    "                    string = \"\"\n",
    "                    for idx,leave in  enumerate(par):\n",
    "                        num = int(idx>0)\n",
    "                        string = string+' '*num + leave[0]\n",
    "                    words_list.append(string)\n",
    "                    \n",
    "                else:\n",
    "                    if not pattern.match(par[0]):\n",
    "                        words_list.append(par[0].lower())\n",
    "\n",
    "f = open('text.json','w')\n",
    "json.dump(words_list, f ,indent = 2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'virtual',\n",
       " '3d',\n",
       " 'model',\n",
       " 'unique',\n",
       " 'to',\n",
       " 'their',\n",
       " 'anatomy',\n",
       " 'using',\n",
       " 'the',\n",
       " 'AR HoloLens',\n",
       " 'or',\n",
       " 'VR Oculus Rift',\n",
       " 'users',\n",
       " 'can',\n",
       " 'see',\n",
       " 'a',\n",
       " 'color-coded',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'a',\n",
       " 'patient',\n",
       " 'bones',\n",
       " 'organs',\n",
       " 'and',\n",
       " 'nerves',\n",
       " 'the',\n",
       " 'software',\n",
       " 'is',\n",
       " 'especially',\n",
       " 'useful',\n",
       " 'to',\n",
       " 'help',\n",
       " 'clinicians',\n",
       " 'visualize',\n",
       " 'the',\n",
       " 'shape',\n",
       " 'and',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'a',\n",
       " 'growth',\n",
       " 'or',\n",
       " 'tumor',\n",
       " 'although',\n",
       " 'various',\n",
       " 'medical',\n",
       " 'institutions',\n",
       " 'have',\n",
       " 'expressed',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'Bosc',\n",
       " 'implementing',\n",
       " 'this',\n",
       " 'technology',\n",
       " 'into',\n",
       " 'a',\n",
       " 'doctor',\n",
       " 'daily',\n",
       " 'clinical',\n",
       " 'workflow',\n",
       " 'will',\n",
       " 'be',\n",
       " 'no',\n",
       " 'small',\n",
       " 'undertaking',\n",
       " 'VR',\n",
       " 'and',\n",
       " 'AR',\n",
       " 'headsets',\n",
       " 'are',\n",
       " 'still',\n",
       " 'in',\n",
       " 'their',\n",
       " 'early',\n",
       " 'stages',\n",
       " 'of',\n",
       " 'development',\n",
       " 'and',\n",
       " 'are',\n",
       " 'prohibitively',\n",
       " 'expensive',\n",
       " 'for',\n",
       " 'some',\n",
       " 'institutions',\n",
       " 'to',\n",
       " 'use',\n",
       " 'on',\n",
       " 'a',\n",
       " 'large',\n",
       " 'scale',\n",
       " 'i',\n",
       " 'think',\n",
       " 'there',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'hype',\n",
       " 'around',\n",
       " 'VR',\n",
       " 'and',\n",
       " 'AR',\n",
       " 'so',\n",
       " 'i',\n",
       " 'think',\n",
       " 'one',\n",
       " 'of',\n",
       " 'our',\n",
       " 'biggest',\n",
       " 'challenges',\n",
       " 'is',\n",
       " 'being',\n",
       " 'able',\n",
       " 'to',\n",
       " 'separate',\n",
       " 'ourselves',\n",
       " 'from',\n",
       " 'that',\n",
       " 'hype',\n",
       " 'and',\n",
       " 'show',\n",
       " 'this',\n",
       " 'can',\n",
       " 'actually',\n",
       " 'provide',\n",
       " 'long-term',\n",
       " 'value',\n",
       " 'to',\n",
       " 'doctors',\n",
       " 'and',\n",
       " 'patients',\n",
       " 'Laughery',\n",
       " 'said',\n",
       " 'the',\n",
       " 'UW Center',\n",
       " 'for',\n",
       " 'Multidimensional Medicine',\n",
       " 'CMM',\n",
       " 'has',\n",
       " 'provided',\n",
       " 'them',\n",
       " 'with',\n",
       " 'the',\n",
       " 'clinical',\n",
       " 'connections',\n",
       " 'to',\n",
       " 'rise',\n",
       " 'to',\n",
       " 'that',\n",
       " 'challenge',\n",
       " 'the',\n",
       " 'UC Irvine Department',\n",
       " 'of',\n",
       " 'Urology',\n",
       " 'was',\n",
       " 'the',\n",
       " 'first',\n",
       " 'institution',\n",
       " 'to',\n",
       " 'partner',\n",
       " 'with',\n",
       " 'Pear Med',\n",
       " 'to',\n",
       " 'see',\n",
       " 'how',\n",
       " 'the',\n",
       " 'VR',\n",
       " 'software',\n",
       " 'could',\n",
       " 'improve',\n",
       " 'surgical',\n",
       " 'training',\n",
       " 'and',\n",
       " 'planning',\n",
       " 'for',\n",
       " 'procedures',\n",
       " 'such',\n",
       " 'as',\n",
       " 'kidney',\n",
       " 'stone',\n",
       " 'removal',\n",
       " 'in',\n",
       " 'November',\n",
       " 'of',\n",
       " 'last',\n",
       " 'year',\n",
       " 'the',\n",
       " 'CMM',\n",
       " 'connected',\n",
       " 'Pear Med',\n",
       " 'with',\n",
       " 'a',\n",
       " 'very',\n",
       " 'unusual',\n",
       " 'case',\n",
       " 'at',\n",
       " 'British Columbia Children',\n",
       " 'Hospital',\n",
       " 'in']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list[300:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import json\n",
    "\n",
    "words_list = []\n",
    "f = open(\"data/data.json\",\"r\")\n",
    "sites = json.load(f)\n",
    "f.close()\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  Company:{<NNP>+}    # chunk determiner/possessive, adjectives and noun\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "pattern = re.compile(r'\\W')\n",
    "\n",
    "content = sites['Geekwire'][0][\"content\"]\n",
    "sentences = nltk.sent_tokenize(content)\n",
    "for sentence in sentences:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    parsers = cp.parse(pos)\n",
    "    pattern = re.compile(r'\\W')\n",
    "    for par in parsers:\n",
    "        if type(par) == nltk.tree.Tree:\n",
    "            string = \"\"\n",
    "            for idx,leave in  enumerate(par):\n",
    "                num = int(idx>0)\n",
    "                string = string+' '*num + leave[0]\n",
    "            words_list.append(string)\n",
    "        else:\n",
    "            if not pattern.match(par[0]):\n",
    "                words_list.append(par[0].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Company Ryan/NNP James/NNP)\n",
      "('(', '(')\n",
      "('left', 'VBN')\n",
      "(')', ')')\n",
      "('and', 'CC')\n",
      "(Company Mark/NNP Laughery/NNP)\n",
      "('(', '(')\n",
      "('right', 'RB')\n",
      "(')', ')')\n",
      "('sit', 'NN')\n",
      "('in', 'IN')\n",
      "('their', 'PRP$')\n",
      "('office', 'NN')\n",
      "('space', 'NN')\n",
      "('at', 'IN')\n",
      "('the', 'DT')\n",
      "(Company UW/NNP CoMotion/NNP Labs/NNP)\n",
      "('.', '.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ryan James',\n",
       " 'left',\n",
       " 'and',\n",
       " 'Mark Laughery',\n",
       " 'right',\n",
       " 'sit',\n",
       " 'in',\n",
       " 'their',\n",
       " 'office',\n",
       " 'space',\n",
       " 'at',\n",
       " 'the',\n",
       " 'UW CoMotion Labs']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list = []\n",
    "for par in parsers:\n",
    "    print(par)\n",
    "    if type(par) == nltk.tree.Tree:\n",
    "        string = \"\"\n",
    "        for idx,leave in  enumerate(par):\n",
    "            num = int(idx>0)\n",
    "            string = string+' '*num + leave[0]\n",
    "        words_list.append(string)\n",
    "            \n",
    "    else:\n",
    "        if not pattern.match(par[0]):\n",
    "            words_list.append(par[0].lower())\n",
    "words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
