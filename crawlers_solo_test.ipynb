{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrepreneur crawling at 20.0%\n",
      "Entrepreneur crawling at 40.0%\n",
      "Entrepreneur crawling at 60.0%\n",
      "Entrepreneur crawling at 80.0%\n",
      "Entrepreneur crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "from crawlers.entrepreneur import CrawlerEntrepreneur\n",
    "import json\n",
    "import time\n",
    "\n",
    "en = CrawlerEntrepreneur(5)\n",
    "en.crawl()\n",
    "articles = en.get_articles()\n",
    "\n",
    "f = open('data/testEN.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu startups crawling at 25.0%\n",
      "eu startups crawling at 50.0%\n",
      "eu startups crawling at 75.0%\n",
      "eu startups crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "from crawlers.eustartups import CrawlerEUStartups\n",
    "import json\n",
    "import time\n",
    "\n",
    "eu = CrawlerEUStartups(4)\n",
    "eu.crawl()\n",
    "articles = eu.get_articles()\n",
    "\n",
    "f = open('data/testEU.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geekwire crawling at 50.0%\n",
      "Geekwire crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "from crawlers.geekwire import CrawlerGeekwire\n",
    "import json\n",
    "import time\n",
    "\n",
    "ge = CrawlerGeekwire(2)\n",
    "ge.crawl()\n",
    "articles = ge.get_articles()\n",
    "\n",
    "f = open('data/testGE.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from crawlers.mashable import CrawlerMashable\n",
    "import json\n",
    "import time\n",
    "\n",
    "ma = CrawlerMashable(2)\n",
    "ma.crawl()\n",
    "articles = ma.get_articles()\n",
    "\n",
    "f = open('data/testMA.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robot_report crawling at 50.0%\n",
      "robot_report crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "from crawlers.robot_report import CrawlerRobotReport\n",
    "import json\n",
    "import time\n",
    "\n",
    "Crawler= CrawlerRobotReport(2)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testRR.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rude Baguette crawling at 50.0%\n",
      "Rude Baguette crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "from crawlers.rudebaguette import CrawlerRudebaguette\n",
    "import json\n",
    "import time\n",
    "\n",
    "Crawler= CrawlerRudebaguette(2)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testRude.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startup Daily Crawling at 50.0%\n",
      "Startup Daily Crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "from crawlers.startupdaily import CrawlerStartupDaily\n",
    "import json\n",
    "import time\n",
    "\n",
    "Crawler= CrawlerStartupDaily(2)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testSD.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startup.uk crawling at 25.0%\n",
      "startup.uk crawling at 50.0%\n",
      "startup.uk crawling at 75.0%\n",
      "startup.uk crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "from crawlers.startupuk import CrawlerStartupsUK\n",
    "import json\n",
    "import time\n",
    "\n",
    "Crawler= CrawlerStartupsUK(4)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testUK.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tech.co crawling at 33.33333333333333%\n",
      "Tech.co crawling at 66.66666666666666%\n",
      "Tech.co crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "from crawlers.tech_co import CrawlerTechCo\n",
    "import json\n",
    "import time\n",
    "\n",
    "Crawler= CrawlerTechCo(3)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testTechCo.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content is not enough or something goes wrong of this link :  https://www.techinasia.com/ping-an-eyes-global-investments-1b-fund\n",
      "skip:  https://www.techinasia.com/talk/vietnamese-startup-went-global\n",
      "skip:  https://www.techinasia.com/stockspot-series-b-funding\n",
      "skip:  https://www.techinasia.com/cartrade-acquires-adroit-inspection\n",
      "skip:  https://www.techinasia.com/8-hardware-startups-for-social-impact\n",
      "skip:  https://www.techinasia.com/singapore-aisg-startups\n",
      "skip:  https://www.techinasia.com/video-jeff-bezos-4-keys-business-success\n",
      "skip:  https://www.techinasia.com/talk/xiaomi-business-model-innovation-netflix\n",
      "skip:  https://www.techinasia.com/talk/tia-worked-3-startups-failed-give\n",
      "skip:  https://www.techinasia.com/talk/startup-grow-own-meat\n"
     ]
    }
   ],
   "source": [
    "from crawlers.tech_in_asia import CrawlerTechInAsia\n",
    "import json\n",
    "import time\n",
    "\n",
    "Crawler= CrawlerTechInAsia(6)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testTechInAsia.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TechInsider crawling at 50.0%\n",
      "TechInsider crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "from crawlers.tech_insider import CrawlerTechInsider\n",
    "import json\n",
    "import time\n",
    "\n",
    "Crawler= CrawlerTechInsider(3)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testTechInsider.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Techcrunch crawling at 33.33333333333333%\n",
      "Techcrunch crawling at 66.66666666666666%\n",
      "Techcrunch crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "from crawlers.techcrunch import CrawlerTechcrunch\n",
    "import json\n",
    "import time\n",
    "\n",
    "Crawler= CrawlerTechcrunch(3)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testTechCrunch.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "venturebeat crawling at 33.33333333333333%\n",
      "venturebeat crawling at 66.66666666666666%\n",
      "venturebeat crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "from crawlers.venturebeat import CrawlerVenturebeat\n",
    "import json\n",
    "import time\n",
    "\n",
    "Crawler= CrawlerVenturebeat(3)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testVB.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VentureBurn crawling at 33.33333333333333%\n",
      "VentureBurn crawling at 66.66666666666666%\n",
      "VentureBurn crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from crawlers.ventureburn import CrawlerVentureBurn\n",
    "\n",
    "Crawler= CrawlerVentureBurn(3)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testVentureBurn.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TechWorld crawling at 33.33333333333333%\n",
      "TechWorld crawling at 66.66666666666666%\n",
      "TechWorld crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from crawlers.techworld import CrawlerTechWorld\n",
    "\n",
    "Crawler= CrawlerTechWorld(3)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testTechWorld.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartupBeat crawling at 33.33333333333333%\n",
      "StartupBeat crawling at 66.66666666666666%\n",
      "StartupBeat crawling at 100.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from crawlers.startupbeat import CrawlerStartupBeat\n",
    "\n",
    "Crawler= CrawlerStartupBeat(3)\n",
    "Crawler.crawl()\n",
    "articles = Crawler.get_articles()\n",
    "\n",
    "f = open('data/testStartupBeat.json','w')\n",
    "json.dump(articles,f,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
