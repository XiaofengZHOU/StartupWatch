{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "import re\n",
    "\n",
    "from crawlers.entrepreneur import CrawlerEntrepreneur\n",
    "from crawlers.eustartups import CrawlerEUStartups\n",
    "from crawlers.geekwire import CrawlerGeekwire\n",
    "from crawlers.mashable import CrawlerMashable\n",
    "from crawlers.robot_report import CrawlerRobotReport\n",
    "from crawlers.startupdaily import CrawlerStartupDaily\n",
    "from crawlers.startupuk import CrawlerStartupsUK\n",
    "from crawlers.tech_co import CrawlerTechCo\n",
    "from crawlers.tech_insider import CrawlerTechInsider\n",
    "from crawlers.techcrunch import CrawlerTechcrunch\n",
    "from crawlers.techworld import CrawlerTechWorld\n",
    "from crawlers.venturebeat import CrawlerVenturebeat\n",
    "from crawlers.ventureburn import CrawlerVentureBurn\n",
    "\n",
    "\n",
    "class RunCrawlers:\n",
    "    def __init__(self, num_pages_by_sites):\n",
    "        self.articles = {}\n",
    "        self.path_database = \"data/data.json\"\n",
    "        self.crawlers = []\n",
    "        self.keys = [\"Techcrunch\",\"Entrepreneur\",\"EUStartups\",\"Geekwire\",\"Mashable\",\"RobotReport\",\"TechInsider\",\n",
    "                     \"TechCo\",\"Venturebeat\",\"StartupDaily\",\"StartupsUK\",\"TechWorld\",\"VentureBurn\"]\n",
    "        \n",
    "        self.setArticles()\n",
    "        self.setCrawlers(num_pages_by_sites)\n",
    "        \n",
    "        \n",
    "    def setArticles(self): \n",
    "        \"\"\"\n",
    "        case1: database file exists \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        case2: database file does not exist\n",
    "        \"\"\"\n",
    "        if os.path.isfile(self.path_database): \n",
    "            with open(self.path_database,'r') as f:\n",
    "                self.articles = json.load(f)    \n",
    "                f.close()\n",
    "                for key in self.keys:\n",
    "                    if key not in self.articles.keys():\n",
    "                        self.articles[key] = []\n",
    "\n",
    "        else:\n",
    "            for key in self.keys:\n",
    "                self.articles[key] = []\n",
    "                \n",
    "    \"\"\"\n",
    "    Using this function, we can choose the crawlers and the number of pages of every crawler.\n",
    "    Example:\n",
    "    num_pages_by_sites ={\"TechCrunch\":5, \"Geekwire\":3}\n",
    "    self.setCrawlers(num_pages_by_sites)\n",
    "    \"\"\"    \n",
    "    def setCrawlers(self,num_pages_by_sites):\n",
    "        for key in num_pages_by_sites.keys():\n",
    "            if key == \"RobotReport\":\n",
    "                self.crawlers.append( CrawlerRobotReport(num_pages_by_sites[\"RobotReport\"]) )\n",
    "            if key == \"Techcrunch\":\n",
    "                self.crawlers.append( CrawlerTechcrunch(num_pages_by_sites[\"Techcrunch\"]) )\n",
    "            if key == \"Mashable\":\n",
    "                self.crawlers.append( CrawlerMashable(num_pages_by_sites[\"Mashable\"]) )\n",
    "            if key == \"Entrepreneur\":\n",
    "                self.crawlers.append( CrawlerEntrepreneur(num_pages_by_sites[\"Entrepreneur\"]) )\n",
    "            if key == \"TechInsider\":\n",
    "                self.crawlers.append( CrawlerTechInsider(num_pages_by_sites[\"TechInsider\"]) )\n",
    "            if key == \"Geekwire\":\n",
    "                self.crawlers.append( CrawlerGeekwire(num_pages_by_sites[\"Geekwire\"]) )\n",
    "            if key == \"TechCo\":\n",
    "                self.crawlers.append( CrawlerTechCo(num_pages_by_sites[\"TechCo\"]) )\n",
    "            if key == \"Venturebeat\":\n",
    "                self.crawlers.append( CrawlerVenturebeat(num_pages_by_sites[\"Venturebeat\"]) )\n",
    "            if key == \"StartupDaily\":\n",
    "                self.crawlers.append( CrawlerStartupDaily(num_pages_by_sites[\"StartupDaily\"]) )\n",
    "            if key == \"EUStartups\":\n",
    "                self.crawlers.append( CrawlerEUStartups(num_pages_by_sites[\"EUStartups\"]) )\n",
    "            if key == \"StartupsUK\":\n",
    "                self.crawlers.append( CrawlerStartupsUK(num_pages_by_sites[\"StartupsUK\"]) )\n",
    "            if key == \"TechWorld\":\n",
    "                self.crawlers.append( CrawlerTechWorld(num_pages_by_sites[\"TechWorld\"]) )\n",
    "            if key == \"VentureBurn\":\n",
    "                self.crawlers.append(CrawlerVentureBurn(num_pages_by_sites[\"VentureBurn\"]) )\n",
    "                \n",
    "    \n",
    "    \"\"\"\n",
    "    Run the crawlers\n",
    "    \"\"\"            \n",
    "    def runCrawlers(self):\n",
    "        articles_total = 0\n",
    "        for crawler in self.crawlers:\n",
    "            crawler.crawl()\n",
    "            articles = crawler.get_articles()\n",
    "            \n",
    "            print(\"*********************************************************\")\n",
    "            filtered_articles = self.filter_startup(articles)   \n",
    "            print(\"number of articles crawled about startups : \", len(filtered_articles) )\n",
    "            filtered_articles = self.filter_by_database(filtered_articles,crawler.name)     \n",
    "            print(\"number of articles crawled to add in the database : \", len(filtered_articles) )\n",
    "            self.articles[crawler.name] = filtered_articles + self.articles[crawler.name]           \n",
    "            articles_total = articles_total+len(filtered_articles)\n",
    "            print(\"number of articles of this site in the database before remove repetition : \", len(self.articles[crawler.name] ) )\n",
    "            \n",
    "            articles_without_repetition =[]\n",
    "            for index,article in enumerate(self.articles[crawler.name]):\n",
    "                if article not in self.articles[crawler.name][index+1:]:\n",
    "                    articles_without_repetition.append(article)\n",
    "            self.articles[crawler.name] = articles_without_repetition\n",
    "            \n",
    "            print(\"number of articles of this site in the database after remove repetition : \", len(self.articles[crawler.name] ) )\n",
    "            print(\"*********************************************************\\n\")\n",
    "            \n",
    "            self.save_articles()\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    the word startup need to be in content or title.\n",
    "    \"\"\"\n",
    "    def filter_startup(self,articles):\n",
    "        pattern = re.compile('\\W')\n",
    "        filtered_articles = []\n",
    "        for article in articles:\n",
    "            title = re.sub( pattern, '', article[\"title\"].lower() )\n",
    "            content = re.sub( pattern, '', article[\"content\"].lower())\n",
    "            if \"startup\" in title or \"startup\" in content:\n",
    "                filtered_articles.append(article)\n",
    "        return filtered_articles\n",
    "    \n",
    "    \"\"\"\n",
    "    avoid adding the same article in the database\n",
    "    \"\"\"\n",
    "    def filter_by_database(self,articles,name_crawler):\n",
    "        pattern = re.compile('\\W')\n",
    "        if name_crawler not in self.articles.keys():\n",
    "            self.articles[name_crawler] = []            \n",
    "        articles_database = self.articles[name_crawler]\n",
    "        if len(articles_database) !=0:\n",
    "            for article_database in articles_database:\n",
    "                for article in articles:\n",
    "                    if re.sub( pattern, '', article[\"title\"].lower() ) == re.sub(pattern,'',article_database[\"title\"].lower()) or article[\"url\"]==article_database[\"url\"]:\n",
    "                        articles.remove(article)                        \n",
    "        return articles\n",
    "        \n",
    "    \n",
    "    def save_articles(self):\n",
    "        with open('data/data.json', 'w') as f:\n",
    "            json.dump(self.articles, f,indent=2)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VentureBurn crawling at 16.666666666666664%\n",
      "VentureBurn crawling at 33.33333333333333%\n",
      "VentureBurn crawling at 50.0%\n",
      "VentureBurn crawling at 66.66666666666666%\n",
      "VentureBurn crawling at 83.33333333333334%\n",
      "VentureBurn crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  62\n",
      "number of articles crawled to add in the database :  62\n",
      "number of articles of this site in the database before remove repetition :  62\n",
      "number of articles of this site in the database after remove repetition :  57\n",
      "*********************************************************\n",
      "\n",
      "TechWorld crawling at 16.666666666666664%\n",
      "TechWorld crawling at 33.33333333333333%\n",
      "TechWorld crawling at 50.0%\n",
      "TechWorld crawling at 66.66666666666666%\n",
      "TechWorld crawling at 83.33333333333334%\n",
      "TechWorld crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  102\n",
      "number of articles crawled to add in the database :  102\n",
      "number of articles of this site in the database before remove repetition :  102\n",
      "number of articles of this site in the database after remove repetition :  102\n",
      "*********************************************************\n",
      "\n",
      "Techcrunch crawling at 10.0%\n",
      "Techcrunch crawling at 20.0%\n",
      "Techcrunch crawling at 30.0%\n",
      "Techcrunch crawling at 40.0%\n",
      "Techcrunch crawling at 50.0%\n",
      "Techcrunch crawling at 60.0%\n",
      "Techcrunch crawling at 70.0%\n",
      "Techcrunch crawling at 80.0%\n",
      "Techcrunch crawling at 90.0%\n",
      "Techcrunch crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  27\n",
      "number of articles crawled to add in the database :  5\n",
      "number of articles of this site in the database before remove repetition :  35\n",
      "number of articles of this site in the database after remove repetition :  35\n",
      "*********************************************************\n",
      "\n",
      "eu startups crawling at 14.285714285714285%\n",
      "eu startups crawling at 28.57142857142857%\n",
      "eu startups crawling at 42.857142857142854%\n",
      "eu startups crawling at 57.14285714285714%\n",
      "eu startups crawling at 71.42857142857143%\n",
      "eu startups crawling at 85.71428571428571%\n",
      "eu startups crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  71\n",
      "number of articles crawled to add in the database :  71\n",
      "number of articles of this site in the database before remove repetition :  71\n",
      "number of articles of this site in the database after remove repetition :  71\n",
      "*********************************************************\n",
      "\n",
      "robot_report crawling at 10.0%\n",
      "robot_report crawling at 20.0%\n",
      "robot_report crawling at 30.0%\n",
      "robot_report crawling at 40.0%\n",
      "robot_report crawling at 50.0%\n",
      "robot_report crawling at 60.0%\n",
      "robot_report crawling at 70.0%\n",
      "robot_report crawling at 80.0%\n",
      "robot_report crawling at 90.0%\n",
      "robot_report crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  89\n",
      "number of articles crawled to add in the database :  10\n",
      "number of articles of this site in the database before remove repetition :  89\n",
      "number of articles of this site in the database after remove repetition :  89\n",
      "*********************************************************\n",
      "\n",
      "venturebeat crawling at 14.285714285714285%\n",
      "venturebeat crawling at 28.57142857142857%\n",
      "venturebeat crawling at 42.857142857142854%\n",
      "venturebeat crawling at 57.14285714285714%\n",
      "venturebeat crawling at 71.42857142857143%\n",
      "venturebeat crawling at 85.71428571428571%\n",
      "venturebeat crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  49\n",
      "number of articles crawled to add in the database :  49\n",
      "number of articles of this site in the database before remove repetition :  49\n",
      "number of articles of this site in the database after remove repetition :  49\n",
      "*********************************************************\n",
      "\n",
      "Tech.co crawling at 14.285714285714285%\n",
      "Tech.co crawling at 28.57142857142857%\n",
      "Tech.co crawling at 42.857142857142854%\n",
      "Tech.co crawling at 57.14285714285714%\n",
      "Tech.co crawling at 71.42857142857143%\n",
      "Tech.co crawling at 85.71428571428571%\n",
      "Tech.co crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  33\n",
      "number of articles crawled to add in the database :  33\n",
      "number of articles of this site in the database before remove repetition :  33\n",
      "number of articles of this site in the database after remove repetition :  33\n",
      "*********************************************************\n",
      "\n",
      "Geekwire crawling at 10.0%\n",
      "Geekwire crawling at 20.0%\n",
      "Geekwire crawling at 30.0%\n",
      "Geekwire crawling at 40.0%\n",
      "Geekwire crawling at 50.0%\n",
      "Geekwire crawling at 60.0%\n",
      "Geekwire crawling at 70.0%\n",
      "Geekwire crawling at 80.0%\n",
      "Geekwire crawling at 90.0%\n",
      "Geekwire crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  271\n",
      "number of articles crawled to add in the database :  271\n",
      "number of articles of this site in the database before remove repetition :  271\n",
      "number of articles of this site in the database after remove repetition :  271\n",
      "*********************************************************\n",
      "\n",
      "Entrepreneur crawling at 12.5%\n",
      "Entrepreneur crawling at 25.0%\n",
      "Entrepreneur crawling at 37.5%\n",
      "Entrepreneur crawling at 50.0%\n",
      "Entrepreneur crawling at 62.5%\n",
      "Entrepreneur crawling at 75.0%\n",
      "Entrepreneur crawling at 87.5%\n",
      "Entrepreneur crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  28\n",
      "number of articles crawled to add in the database :  8\n",
      "number of articles of this site in the database before remove repetition :  36\n",
      "number of articles of this site in the database after remove repetition :  36\n",
      "*********************************************************\n",
      "\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  41\n",
      "number of articles crawled to add in the database :  36\n",
      "number of articles of this site in the database before remove repetition :  40\n",
      "number of articles of this site in the database after remove repetition :  39\n",
      "*********************************************************\n",
      "\n",
      "startup.uk crawling at 10.0%\n",
      "startup.uk crawling at 20.0%\n",
      "startup.uk crawling at 30.0%\n",
      "startup.uk crawling at 40.0%\n",
      "startup.uk crawling at 50.0%\n",
      "startup.uk crawling at 60.0%\n",
      "startup.uk crawling at 70.0%\n",
      "startup.uk crawling at 80.0%\n",
      "startup.uk crawling at 90.0%\n",
      "startup.uk crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  91\n",
      "number of articles crawled to add in the database :  91\n",
      "number of articles of this site in the database before remove repetition :  91\n",
      "number of articles of this site in the database after remove repetition :  91\n",
      "*********************************************************\n",
      "\n",
      "TechInsider crawling at 11.11111111111111%\n",
      "TechInsider crawling at 22.22222222222222%\n",
      "TechInsider crawling at 33.33333333333333%\n",
      "TechInsider crawling at 44.44444444444444%\n",
      "TechInsider crawling at 55.55555555555556%\n",
      "TechInsider crawling at 66.66666666666666%\n",
      "TechInsider crawling at 77.77777777777779%\n",
      "TechInsider crawling at 88.88888888888889%\n",
      "TechInsider crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  17\n",
      "number of articles crawled to add in the database :  17\n",
      "number of articles of this site in the database before remove repetition :  17\n",
      "number of articles of this site in the database after remove repetition :  17\n",
      "*********************************************************\n",
      "\n",
      "Startup Daily Crawling at 14.285714285714285%\n",
      "Startup Daily Crawling at 28.57142857142857%\n",
      "Startup Daily Crawling at 42.857142857142854%\n",
      "Startup Daily Crawling at 57.14285714285714%\n",
      "Startup Daily Crawling at 71.42857142857143%\n",
      "Startup Daily Crawling at 85.71428571428571%\n",
      "Startup Daily Crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  85\n",
      "number of articles crawled to add in the database :  85\n",
      "number of articles of this site in the database before remove repetition :  85\n",
      "number of articles of this site in the database after remove repetition :  85\n",
      "*********************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sites = {\"RobotReport\":10,\"Techcrunch\":10,\"Mashable\":10,\"Entrepreneur\":8,\"TechInsider\":10,\n",
    "        \"Geekwire\":10,\"TechCo\":7,\"Venturebeat\":7,\"StartupDaily\":7,\"EUStartups\":7,\"StartupsUK\":10,\"TechWorld\":6,\"VentureBurn\":6}\n",
    "\n",
    "runCrawlers = RunCrawlers(sites)\n",
    "runCrawlers.runCrawlers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runCrawlers.save_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/data.json','r') as f:\n",
    "    articles = json.load(f)    \n",
    "    f.close()\n",
    "    keys = articles.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VentureBurn 57\n",
      "TechWorld 102\n",
      "Techcrunch 35\n",
      "EUStartups 71\n",
      "RobotReport 89\n",
      "Venturebeat 49\n",
      "TechCo 33\n",
      "Entrepreneur 36\n",
      "Mashable 39\n",
      "TechInsider 17\n",
      "StartupsUK 91\n",
      "Geekwire 271\n",
      "StartupDaily 85\n"
     ]
    }
   ],
   "source": [
    "for key in keys:\n",
    "    print(key,len(articles[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
