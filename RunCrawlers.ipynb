{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "import re\n",
    "\n",
    "from crawlers.entrepreneur import CrawlerEntrepreneur\n",
    "from crawlers.eustartups import CrawlerEUStartups\n",
    "from crawlers.geekwire import CrawlerGeekwire\n",
    "from crawlers.mashable import CrawlerMashable\n",
    "from crawlers.robot_report import CrawlerRobotReport\n",
    "from crawlers.startupbeat import CrawlerStartupBeat\n",
    "from crawlers.startupdaily import CrawlerStartupDaily\n",
    "from crawlers.startupuk import CrawlerStartupsUK\n",
    "from crawlers.tech_co import CrawlerTechCo\n",
    "from crawlers.tech_insider import CrawlerTechInsider\n",
    "from crawlers.techcrunch import CrawlerTechcrunch\n",
    "from crawlers.techworld import CrawlerTechWorld\n",
    "from crawlers.venturebeat import CrawlerVenturebeat\n",
    "from crawlers.ventureburn import CrawlerVentureBurn\n",
    "\n",
    "\n",
    "class RunCrawlers:\n",
    "    def __init__(self, num_pages_by_sites):\n",
    "        self.articles = {}\n",
    "        self.path_database = \"data/data.json\"\n",
    "        self.crawlers = []\n",
    "        self.keys = [\"Techcrunch\",\"Entrepreneur\",\"EUStartups\",\"Geekwire\",\"Mashable\",\"RobotReport\",\"TechInsider\",\n",
    "                     \"TechCo\",\"Venturebeat\",\"StartupDaily\",\"StartupsUK\",\"TechWorld\",\"VentureBurn\",\"StartupBeat\"]\n",
    "        \n",
    "        self.setArticles()\n",
    "        self.setCrawlers(num_pages_by_sites)\n",
    "        \n",
    "        \n",
    "    def setArticles(self): \n",
    "        \"\"\"\n",
    "        case1: database file exists \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        case2: database file does not exist\n",
    "        \"\"\"\n",
    "        if os.path.isfile(self.path_database): \n",
    "            with open(self.path_database,'r') as f:\n",
    "                self.articles = json.load(f)    \n",
    "                f.close()\n",
    "                for key in self.keys:\n",
    "                    if key not in self.articles.keys():\n",
    "                        self.articles[key] = []\n",
    "\n",
    "        else:\n",
    "            for key in self.keys:\n",
    "                self.articles[key] = []\n",
    "                \n",
    "    \"\"\"\n",
    "    Using this function, we can choose the crawlers and the number of pages of every crawler.\n",
    "    Example:\n",
    "    num_pages_by_sites ={\"TechCrunch\":5, \"Geekwire\":3}\n",
    "    self.setCrawlers(num_pages_by_sites)\n",
    "    \"\"\"    \n",
    "    def setCrawlers(self,num_pages_by_sites):\n",
    "        for key in num_pages_by_sites.keys():\n",
    "            if key == \"RobotReport\":\n",
    "                self.crawlers.append( CrawlerRobotReport(num_pages_by_sites[\"RobotReport\"]) )\n",
    "            if key == \"Techcrunch\":\n",
    "                self.crawlers.append( CrawlerTechcrunch(num_pages_by_sites[\"Techcrunch\"]) )\n",
    "            if key == \"Mashable\":\n",
    "                self.crawlers.append( CrawlerMashable(num_pages_by_sites[\"Mashable\"]) )\n",
    "            if key == \"Entrepreneur\":\n",
    "                self.crawlers.append( CrawlerEntrepreneur(num_pages_by_sites[\"Entrepreneur\"]) )\n",
    "            if key == \"TechInsider\":\n",
    "                self.crawlers.append( CrawlerTechInsider(num_pages_by_sites[\"TechInsider\"]) )\n",
    "            if key == \"Geekwire\":\n",
    "                self.crawlers.append( CrawlerGeekwire(num_pages_by_sites[\"Geekwire\"]) )\n",
    "            if key == \"TechCo\":\n",
    "                self.crawlers.append( CrawlerTechCo(num_pages_by_sites[\"TechCo\"]) )\n",
    "            if key == \"Venturebeat\":\n",
    "                self.crawlers.append( CrawlerVenturebeat(num_pages_by_sites[\"Venturebeat\"]) )\n",
    "            if key == \"StartupDaily\":\n",
    "                self.crawlers.append( CrawlerStartupDaily(num_pages_by_sites[\"StartupDaily\"]) )\n",
    "            if key == \"EUStartups\":\n",
    "                self.crawlers.append( CrawlerEUStartups(num_pages_by_sites[\"EUStartups\"]) )\n",
    "            if key == \"StartupsUK\":\n",
    "                self.crawlers.append( CrawlerStartupsUK(num_pages_by_sites[\"StartupsUK\"]) )\n",
    "            if key == \"TechWorld\":\n",
    "                self.crawlers.append( CrawlerTechWorld(num_pages_by_sites[\"TechWorld\"]) )\n",
    "            if key == \"VentureBurn\":\n",
    "                self.crawlers.append(CrawlerVentureBurn(num_pages_by_sites[\"VentureBurn\"]) )\n",
    "            if key == \"StartupBeat\":\n",
    "                self.crawlers.append(CrawlerStartupBeat(num_pages_by_sites[\"StartupBeat\"]) )\n",
    "                \n",
    "                \n",
    "    \n",
    "    \"\"\"\n",
    "    Run the crawlers\n",
    "    \"\"\"            \n",
    "    def runCrawlers(self):\n",
    "        articles_total = 0\n",
    "        for crawler in self.crawlers:\n",
    "            crawler.crawl()\n",
    "            articles = crawler.get_articles()\n",
    "            \n",
    "            print(\"*********************************************************\")\n",
    "            filtered_articles = self.filter_startup(articles)   \n",
    "            print(\"number of articles crawled about startups : \", len(filtered_articles) )\n",
    "            filtered_articles = self.filter_by_database(filtered_articles,crawler.name)     \n",
    "            print(\"number of articles crawled to add in the database : \", len(filtered_articles) )\n",
    "            self.articles[crawler.name] = filtered_articles + self.articles[crawler.name]           \n",
    "            articles_total = articles_total+len(filtered_articles)\n",
    "            print(\"number of articles of this site in the database before remove repetition : \", len(self.articles[crawler.name] ) )\n",
    "            \n",
    "            articles_without_repetition =[]\n",
    "            for index,article in enumerate(self.articles[crawler.name]):\n",
    "                if article not in self.articles[crawler.name][index+1:]:\n",
    "                    articles_without_repetition.append(article)\n",
    "            self.articles[crawler.name] = articles_without_repetition\n",
    "            \n",
    "            print(\"number of articles of this site in the database after remove repetition : \", len(self.articles[crawler.name] ) )\n",
    "            print(\"*********************************************************\\n\")\n",
    "            \n",
    "            self.save_articles()\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    the word startup need to be in content or title.\n",
    "    \"\"\"\n",
    "    def filter_startup(self,articles):\n",
    "        pattern = re.compile('\\W')\n",
    "        filtered_articles = []\n",
    "        for article in articles:\n",
    "            title = re.sub( pattern, '', article[\"title\"].lower() )\n",
    "            content = re.sub( pattern, '', article[\"content\"].lower())\n",
    "            if \"startup\" in title or \"startup\" in content:\n",
    "                filtered_articles.append(article)\n",
    "        return filtered_articles\n",
    "    \n",
    "    \"\"\"\n",
    "    avoid adding the same article in the database\n",
    "    \"\"\"\n",
    "    def filter_by_database(self,articles,name_crawler):\n",
    "        pattern = re.compile('\\W')\n",
    "        if name_crawler not in self.articles.keys():\n",
    "            self.articles[name_crawler] = []            \n",
    "        articles_database = self.articles[name_crawler]\n",
    "        if len(articles_database) !=0:\n",
    "            for article_database in articles_database:\n",
    "                for article in articles:\n",
    "                    if re.sub( pattern, '', article[\"title\"].lower() ) == re.sub(pattern,'',article_database[\"title\"].lower()) or article[\"url\"]==article_database[\"url\"]:\n",
    "                        articles.remove(article)                        \n",
    "        return articles\n",
    "        \n",
    "    \n",
    "    def save_articles(self):\n",
    "        with open('data/data.json', 'w') as f:\n",
    "            json.dump(self.articles, f,indent=2)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tech.co crawling at 20.0%\n",
      "Tech.co crawling at 40.0%\n",
      "Tech.co crawling at 60.0%\n",
      "Tech.co crawling at 80.0%\n",
      "Tech.co crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  56\n",
      "number of articles crawled to add in the database :  26\n",
      "number of articles of this site in the database before remove repetition :  563\n",
      "number of articles of this site in the database after remove repetition :  563\n",
      "*********************************************************\n",
      "\n",
      "startup.uk crawling at 50.0%\n",
      "startup.uk crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  18\n",
      "number of articles crawled to add in the database :  2\n",
      "number of articles of this site in the database before remove repetition :  30\n",
      "number of articles of this site in the database after remove repetition :  30\n",
      "*********************************************************\n",
      "\n",
      "venturebeat crawling at 20.0%\n",
      "venturebeat crawling at 40.0%\n",
      "venturebeat crawling at 60.0%\n",
      "venturebeat crawling at 80.0%\n",
      "venturebeat crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  27\n",
      "number of articles crawled to add in the database :  22\n",
      "number of articles of this site in the database before remove repetition :  102\n",
      "number of articles of this site in the database after remove repetition :  102\n",
      "*********************************************************\n",
      "\n",
      "Entrepreneur crawling at 20.0%\n",
      "Entrepreneur crawling at 40.0%\n",
      "Entrepreneur crawling at 60.0%\n",
      "Entrepreneur crawling at 80.0%\n",
      "Entrepreneur crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  22\n",
      "number of articles crawled to add in the database :  4\n",
      "number of articles of this site in the database before remove repetition :  155\n",
      "number of articles of this site in the database after remove repetition :  155\n",
      "*********************************************************\n",
      "\n",
      "TechInsider crawling at 25.0%\n",
      "TechInsider crawling at 50.0%\n",
      "TechInsider crawling at 75.0%\n",
      "TechInsider crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  31\n",
      "number of articles crawled to add in the database :  28\n",
      "number of articles of this site in the database before remove repetition :  122\n",
      "number of articles of this site in the database after remove repetition :  122\n",
      "*********************************************************\n",
      "\n",
      "TechWorld crawling at 50.0%\n",
      "TechWorld crawling at 100.0%\n",
      "*********************************************************\n",
      "number of articles crawled about startups :  36\n",
      "number of articles crawled to add in the database :  5\n",
      "number of articles of this site in the database before remove repetition :  70\n",
      "number of articles of this site in the database after remove repetition :  70\n",
      "*********************************************************\n",
      "\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:720)>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m                 \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers)\u001b[0m\n\u001b[0;32m   1106\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers)\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body)\u001b[0m\n\u001b[0;32m   1102\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    876\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1260\u001b[0m             self.sock = self._context.wrap_socket(self.sock,\n\u001b[1;32m-> 1261\u001b[1;33m                                                   server_hostname=server_hostname)\n\u001b[0m\u001b[0;32m   1262\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_hostname\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_hostname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)\u001b[0m\n\u001b[0;32m    384\u001b[0m                          \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                          _context=self)\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\ssl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context)\u001b[0m\n\u001b[0;32m    759\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m    995\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 996\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    997\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;34m\"\"\"Start the SSL/TLS handshake.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_hostname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSSLError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:720)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-215ebd0536a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#sites = {\"Venturebeat\":5}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrunCrawlers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRunCrawlers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msites\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mrunCrawlers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunCrawlers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mrunCrawlers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_articles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f6f052d76742>\u001b[0m in \u001b[0;36mrunCrawlers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0marticles_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcrawler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m             \u001b[0marticles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_articles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xizhou\\Desktop\\StartupWatch\\crawlers\\robot_report.py\u001b[0m in \u001b[0;36mcrawl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0mhtml_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_html_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mhtml_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Failure in getting HTML doc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\xizhou\\Desktop\\StartupWatch\\crawlers\\tools.py\u001b[0m in \u001b[0;36mget_html_doc\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mhtml_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemoteDisconnected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 484\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[1;32m-> 1297\u001b[1;33m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\xizhou\\appdata\\local\\programs\\python\\python35\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1254\u001b[0m                 \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1257\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:720)>"
     ]
    }
   ],
   "source": [
    "sites = {\"RobotReport\":1,\"Techcrunch\":5,\"Mashable\":4,\"Entrepreneur\":5,\"TechInsider\":5,\n",
    "        \"Geekwire\":2,\"TechCo\":5,\"Venturebeat\":5,\"StartupDaily\":5,\"EUStartups\":5,\"StartupsUK\":2,\"TechWorld\":2,\"VentureBurn\":5}\n",
    "\n",
    "#sites = {\"Venturebeat\":5}\n",
    "runCrawlers = RunCrawlers(sites)\n",
    "runCrawlers.runCrawlers()\n",
    "runCrawlers.save_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "f=open('data/data.json','r')\n",
    "articles = json.load(f)    \n",
    "f.close()\n",
    "keys = articles.keys()\n",
    "for key in keys:  \n",
    "    print(key,len(articles[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartupBeat 65\n",
      "VentureBurn 204\n",
      "Venturebeat 48\n",
      "Entrepreneur 143\n",
      "Techcrunch 148\n",
      "Geekwire 137\n",
      "StartupDaily 256\n",
      "EUStartups 149\n",
      "Mashable 59\n",
      "StartupsUK 28\n",
      "RobotReport 16\n",
      "TechCo 514\n",
      "TechWorld 62\n",
      "TechInsider 77\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f=open('data/data.json','r')\n",
    "articles = json.load(f)    \n",
    "f.close()\n",
    "keys = articles.keys()\n",
    "for key in keys:  \n",
    "    print(key,len(articles[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
